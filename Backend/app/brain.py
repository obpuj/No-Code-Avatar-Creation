import os
from typing import TypedDict, Literal, Optional
from langgraph.graph import StateGraph, END

# LangChain Imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_groq import ChatGroq
load_dotenv()

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise RuntimeError("No GROQ_API_KEY found.")

llm_flash = ChatGroq(model="llama-3.3-70b-versatile", api_key=GROQ_API_KEY, temperature=0)
llm_behavior = ChatGroq(model="llama-3.3-70b-versatile", api_key=GROQ_API_KEY, temperature=0)

class AgentState(TypedDict):
    """The shared memory passed between all agents."""
    user_input: str
    persona_prompt: str      # The personality instructions
    knowledge_context: str   # The content of the uploaded PDF/Doc
    intent: str              # Classified by Orchestrator (CHAT or END)
    response_text: str       # Generated by Narrative
    is_grounded: bool        # Checked by Hallucination Grader
    behavior_json: dict      # Generated by Behavior

# --- OUTPUT MODELS (PYDANTIC) ---

class GradeHallucinations(BaseModel):
    """Binary score for hallucination check."""
    binary_score: str = Field(description="'yes' if the answer is grounded in the documents, 'no' if it is a hallucination")

class AnimationSignal(BaseModel):
    """Strict JSON output for the frontend animation engine."""
    emotion: str = Field(description="One of: neutral, happy, sad, angry, surprised, confused")
    gesture: str = Field(description="One of: idle, wave, bow, talk_excited, shrug, scratch_head")

# --- PROMPTS ---

ORCHESTRATOR_PROMPT = """
ROLE:
You are the Router for an advanced AI Avatar system. Your ONLY job is to classify the user's input into a specific category. Do not answer the user. Do not be polite. Just classify.

CATEGORIES:
1. CHAT: The user is asking a question, making a statement, or starting a conversation. (e.g., "Hello", "How does this work?", "I am angry")
2. END: The user wants to stop the interaction. (e.g., "Goodbye", "Exit", "Shut down", "Stop", "See you later")
3. SAFETY_BLOCK: The user is asking for illegal, harmful, or explicit content.

INSTRUCTIONS:
- Analyze the input below.
- Return ONLY the category name (CHAT, END, or SAFETY_BLOCK).
- Do not add punctuation or explanation.

USER INPUT:
{user_input}
"""

NARRATIVE_PROMPT = """
IDENTITY & PERSONA:
{persona_prompt}

KNOWLEDGE BASE (TRUTH SOURCE):
{knowledge_context}

INSTRUCTIONS:
1. You are the avatar described in the "IDENTITY" section. You must stay in character at all times.
2. Your answers must be based PRIMARILY on the "KNOWLEDGE BASE" provided above.
3. If the user asks a specific factual question (e.g., "What is your refund policy?") and the answer is NOT in the Knowledge Base, you must admit you do not know. Do not make up facts.
4. Keep your responses short and spoken-style (under 3 sentences). Long blocks of text look bad on a 3D avatar.
5. Do not use emojis. (The 3D model cannot render them).

USER MESSAGE:
{user_input}
"""

GRADER_PROMPT = """
You are a lenient fact-checker. 
Compare the [Document], [User Input], and the [Response].

[Document]: 
{knowledge_context}

[User Input]:
{user_input}

[Response]: 
{response_text}

TASK:
Determine if the Response is consistent with the Document.
- If the User Input is a greeting (hello, hi, hey, greetings, good morning, etc.) AND the Response is a greeting response, score 'yes' (greetings don't need to be in the document).
- If the Response is just polite chit-chat ("Hello", "Sorry", "Thank you", "You're welcome"), score 'yes'.
- If the Response implies facts not found in the Document, score 'no'.
- If the Response matches the Document, score 'yes'.

Be generous. If you are unsure, score 'yes'.
"""

BEHAVIOR_PROMPT = """
ROLE:
You are the Animation Director for a 3D character. You do not write text; you output JSON instructions for the body and face.

USER INPUT (What the user said):
"{user_input}"

AVATAR RESPONSE (What the avatar is saying):
"{response_text}"

AVAILABLE ANIMATIONS (Strict List):
- Gestures: [idle, wave, talk]
- Emotions: [neutral, happy, sad, angry, surprised, confused]

INSTRUCTIONS:
1. Analyze BOTH the user input and the avatar response to determine the context.
2. Select the ONE best gesture and ONE best emotion from the lists above.
3. Rules for Selection (check in this order - FIRST match wins):
   - If user input contains greeting words (hello, hi, hey, greetings, good morning, good afternoon, good evening) OR avatar response is clearly a greeting response -> gesture: "wave", emotion: "happy"
   - If explaining/teaching/answering questions -> gesture: "talk", emotion: "neutral"
   - Default/Generic -> gesture: "talk", emotion: "neutral"

IMPORTANT: If the user says "hello", "hi", "hey", or any greeting, you MUST return gesture: "wave" regardless of what the avatar response says.

OUTPUT FORMAT:
You must return ONLY a raw JSON object. Do not wrap it in markdown blocks.
{{
  "emotion": "selected_emotion",
  "gesture": "selected_gesture"
}}
"""
# --- AGENT NODES ---

def orchestrator_node(state: AgentState):
    """Classifies user intent."""
    prompt = ChatPromptTemplate.from_template(ORCHESTRATOR_PROMPT)
    chain = prompt | llm_flash
    result = chain.invoke({"user_input": state["user_input"]})
    
    cleaned_intent = result.content.strip().upper()
    if cleaned_intent not in ["CHAT", "END"]:
        cleaned_intent = "CHAT" # Fallback
        
    print(f"--- ORCHESTRATOR: Intent is {cleaned_intent} ---")
    return {"intent": cleaned_intent}

def narrative_node(state: AgentState):
    """Generates the text response."""
    prompt = ChatPromptTemplate.from_template(NARRATIVE_PROMPT)
    chain = prompt | llm_flash
    result = chain.invoke({
        "persona_prompt": state["persona_prompt"],
        "knowledge_context": state.get("knowledge_context", "No context provided."),
        "user_input": state["user_input"]
    })
    
    print(f"--- NARRATIVE: Generated text ---")
    return {"response_text": result.content}

def hallucination_check_node(state: AgentState):
    """Verifies if the text matches the knowledge base."""
    structured_llm = llm_flash.with_structured_output(GradeHallucinations)
    prompt = ChatPromptTemplate.from_template(GRADER_PROMPT)
    chain = prompt | structured_llm
    
    score = chain.invoke({
        "knowledge_context": state.get("knowledge_context", ""),
        "user_input": state.get("user_input", ""),
        "response_text": state["response_text"]
    })
    
    is_grounded = score.binary_score == 'yes'
    final_text = state['response_text']
    
    # If hallucination detected, override text
    if not is_grounded:
        print("--- GRADER: Hallucination Detected! Overriding. ---")
        final_text = "I apologize, but I cannot verify that information based on my internal guidelines."
    else:
        print("--- GRADER: Check Passed. ---")
        
    return {
        "is_grounded": is_grounded,
        "response_text": final_text
    }

def behavior_node(state: AgentState):
    """Generates JSON for gestures."""
    structured_llm = llm_behavior.with_structured_output(AnimationSignal)
    prompt = ChatPromptTemplate.from_template(BEHAVIOR_PROMPT)
    chain = prompt | structured_llm
    
    result = chain.invoke({
        "user_input": state.get("user_input", ""),
        "response_text": state["response_text"]
    })
    
    print(f"--- BEHAVIOR: {result.json()} ---")
    return {"behavior_json": result.dict()}

def end_node(state: AgentState):
    """Hardcoded exit response."""
    return {
        "response_text": "Goodbye! Have a great day.",
        "behavior_json": {"emotion": "happy", "gesture": "wave"}
    }

# --- GRAPH CONSTRUCTION ---

def route_decision(state: AgentState):
    if state["intent"] == "END":
        return "end_conversation"
    return "narrative"

workflow = StateGraph(AgentState)

# Add Nodes
workflow.add_node("orchestrator", orchestrator_node)
workflow.add_node("narrative", narrative_node)
workflow.add_node("hallucination_check", hallucination_check_node)
workflow.add_node("behavior", behavior_node)
workflow.add_node("end_conversation", end_node)

# Entry Point
workflow.set_entry_point("orchestrator")

# Edges
workflow.add_conditional_edges(
    "orchestrator",
    route_decision,
    {
        "narrative": "narrative",
        "end_conversation": "end_conversation"
    }
)

workflow.add_edge("narrative", "hallucination_check")
workflow.add_edge("hallucination_check", "behavior")
workflow.add_edge("behavior", END)
workflow.add_edge("end_conversation", END)

# Compile Application
brain_app = workflow.compile()

# --- HELPER FOR FASTAPI ---

async def run_chat_brain(user_input: str, persona_key: str = None, context_text: str = "", persona_prompt: str = None):
    """
    Main entry point to be called by FastAPI.
    
    Args:
        user_input: The user's prompt/question
        persona_key: Optional persona ID to lookup (e.g., "professional", "sarcastic")
        context_text: The knowledge context/knowledge base text
        persona_prompt: Optional direct persona prompt (overrides persona_key if provided)
    """
    # Lookup Table for Personas (Hardcoded for Hackathon)
    PERSONAS = {
        "sarcastic": "You are a sarcastic tech support agent. You are helpful but slightly rude.",
        "professional": "You are a polite, corporate customer service representative.",
        "excited": "You are a super high-energy sales rep! Use lots of exclamation marks."
    }
    
    # Use direct persona_prompt if provided, otherwise lookup by persona_key
    final_persona_prompt = persona_prompt
    if not final_persona_prompt:
        final_persona_prompt = PERSONAS.get(persona_key or "professional", PERSONAS["professional"])
    
    inputs = {
        "user_input": user_input,
        "persona_prompt": final_persona_prompt,
        "knowledge_context": context_text
    }
    
    # Run the graph
    result = await brain_app.ainvoke(inputs)
    
    return {
        "text": result["response_text"],
        "behavior": result["behavior_json"]
    }

